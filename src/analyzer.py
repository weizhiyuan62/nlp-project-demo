"""
æ™ºèƒ½ä¿¡æ¯åˆ†ææ¨¡å—
ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹å¯¹é‡‡é›†çš„ä¿¡æ¯è¿›è¡Œè¯„åˆ†ã€ç­›é€‰å’Œå…³è”åˆ†æ
"""

import logging
import json
import requests
from typing import List, Dict, Any, Tuple
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# ç¾è§‚çš„å¤šè¿›åº¦æ¡æ˜¾ç¤º
from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn, TaskProgressColumn, TimeElapsedColumn, TimeRemainingColumn
from rich.console import Console
from rich.live import Live
from rich.table import Table
from rich.panel import Panel


class InformationAnalyzer:
    """ä¿¡æ¯æ™ºèƒ½åˆ†æå™¨"""
    
    def __init__(self, config_manager):
        """
        åˆå§‹åŒ–ä¿¡æ¯åˆ†æå™¨
        
        Args:
            config_manager: é…ç½®ç®¡ç†å™¨å®ä¾‹
        """
        self.config = config_manager
        self.logger = logging.getLogger(f"æ™ºè§ˆç³»ç»Ÿv{config_manager.version}")
        self.llm_config = config_manager.get_api_config('llm')
        self.scoring_weights = config_manager.get('analysis', 'scoring', default={})
        self.min_score = config_manager.get('analysis', 'min_score', default=0.6)
    
    def analyze_items(self, items: List[Dict[str, Any]], topics: List[str]) -> Dict[str, Any]:
        """
        å¯¹é‡‡é›†çš„ä¿¡æ¯è¿›è¡Œå…¨é¢åˆ†æ
        
        Args:
            items: å¾…åˆ†æçš„ä¿¡æ¯åˆ—è¡¨
            topics: ä¸»é¢˜åˆ—è¡¨
            
        Returns:
            åˆ†æç»“æœå­—å…¸ï¼ŒåŒ…å«è¯„åˆ†ã€ç­›é€‰åçš„ä¿¡æ¯ã€å…³é”®è¦ç‚¹ç­‰
        """
        self.logger.info(f"å¼€å§‹åˆ†æ {len(items)} æ¡ä¿¡æ¯")
        
        if not items:
            self.logger.warning("æ²¡æœ‰ä¿¡æ¯éœ€è¦åˆ†æ")
            return {
                'scored_items': [],
                'filtered_items': [],
                'key_points': [],
                'relationships': [],
                'statistics': {},
                'analysis_time': datetime.now().isoformat()
            }
        
        # æ‰¹é‡è¯„åˆ†
        scored_items = self._score_items_batch(items, topics)
        
        # ç­›é€‰é«˜åˆ†ä¿¡æ¯
        filtered_items = [item for item in scored_items if item.get('score', 0) >= self.min_score]
        self.logger.info(f"ç­›é€‰åä¿ç•™ {len(filtered_items)} æ¡é«˜è´¨é‡ä¿¡æ¯")
        
        # æå–å…³é”®è¦ç‚¹
        key_points = self._extract_key_points(filtered_items, topics)
        
        # è¯†åˆ«å…³è”å…³ç³»
        relationships = self._identify_relationships(filtered_items)
        
        # ç»Ÿè®¡åˆ†æ
        statistics = self._compute_statistics(filtered_items)
        
        # ç”Ÿæˆæ€»ä½“åˆ†æï¼ˆä½¿ç”¨LLMæ•´åˆæ‰€æœ‰ä¿¡æ¯ç”Ÿæˆæ·±åº¦åˆ†æï¼‰
        overall_analysis = self._generate_overall_analysis(filtered_items, key_points, topics, statistics)
        
        result = {
            'scored_items': scored_items,
            'filtered_items': filtered_items,
            'key_points': key_points,
            'relationships': relationships,
            'statistics': statistics,
            'overall_analysis': overall_analysis,
            'analysis_time': datetime.now().isoformat()
        }
        
        return result
    
    def _score_items_batch(self, items: List[Dict[str, Any]], topics: List[str]) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡å¯¹ä¿¡æ¯è¿›è¡Œè¯„åˆ†ï¼ˆå¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†ï¼Œå¸¦ç¾è§‚å¤šè¿›åº¦æ¡æ˜¾ç¤ºï¼‰
        
        Args:
            items: ä¿¡æ¯åˆ—è¡¨
            topics: ä¸»é¢˜åˆ—è¡¨
            
        Returns:
            å¸¦è¯„åˆ†çš„ä¿¡æ¯åˆ—è¡¨
        """
        self.logger.info("å¼€å§‹æ‰¹é‡è¯„åˆ†ï¼ˆå¤šçº¿ç¨‹å¹¶è¡Œï¼‰...")
        
        batch_size = 10  # æ¯æ‰¹å¤„ç†10æ¡ä¿¡æ¯
        max_workers = 5  # å¹¶è¡Œçº¿ç¨‹æ•°ï¼Œå¯æ ¹æ® API é™æµè°ƒæ•´
        
        # å°† items åˆ†æˆå¤šä¸ª batch
        batches = []
        for i in range(0, len(items), batch_size):
            batch = items[i:i+batch_size]
            batches.append((i, batch))  # è®°å½•åŸå§‹ç´¢å¼•å’Œæ‰¹æ¬¡æ•°æ®
        
        total_batches = len(batches)
        
        # ç”¨äºå­˜æ”¾ç»“æœçš„å­—å…¸ï¼ŒæŒ‰åŸå§‹ç´¢å¼•æ’åº
        results_dict: Dict[int, List[Dict[str, Any]]] = {}
        results_lock = threading.Lock()
        
        # ç”¨äºè·Ÿè¸ªæ¯ä¸ª worker çš„çŠ¶æ€
        worker_status: Dict[int, Dict[str, Any]] = {}
        status_lock = threading.Lock()
        
        console = Console()
        
        def score_single_batch(batch_info: Tuple[int, List[Dict[str, Any]]], worker_id: int, 
                              progress: Progress, task_ids: Dict[int, Any], overall_task: Any) -> None:
            """çº¿ç¨‹ä»»åŠ¡ï¼šè¯„åˆ†å•ä¸ªæ‰¹æ¬¡"""
            batch_start_idx, batch = batch_info
            batch_num = batch_start_idx // batch_size + 1
            
            # æ›´æ–° worker è¿›åº¦æ¡æè¿°
            with status_lock:
                worker_status[worker_id] = {'batch': batch_num, 'status': 'å¤„ç†ä¸­'}
            progress.update(task_ids[worker_id], description=f"[cyan]Worker {worker_id+1}[/] æ‰¹æ¬¡ {batch_num}/{total_batches}")
            progress.start_task(task_ids[worker_id])
            
            try:
                batch_scores = self._call_llm_for_scoring(batch, topics)
                
                scored_batch = []
                for j, item in enumerate(batch):
                    item_copy = item.copy()
                    if j < len(batch_scores):
                        item_copy.update(batch_scores[j])
                    else:
                        # å¦‚æœLLMè¿”å›ç»“æœä¸è¶³ï¼Œä½¿ç”¨é»˜è®¤è¯„åˆ†
                        item_copy['score'] = 0.5
                        item_copy['relevance'] = 0.65
                        item_copy['importance'] = 0.5
                        item_copy['timeliness'] = 0.5
                        item_copy['reliability'] = 0.5
                    scored_batch.append(item_copy)
                
                with results_lock:
                    results_dict[batch_start_idx] = scored_batch
                
                # æ›´æ–° worker çŠ¶æ€ä¸ºå®Œæˆ
                with status_lock:
                    worker_status[worker_id] = {'batch': batch_num, 'status': 'å®Œæˆ'}
                progress.update(task_ids[worker_id], completed=100, 
                              description=f"[green]Worker {worker_id+1}[/] æ‰¹æ¬¡ {batch_num} âœ“")
                    
            except Exception as e:
                self.logger.error(f"æ‰¹æ¬¡è¯„åˆ†å¤±è´¥ (èµ·å§‹ç´¢å¼• {batch_start_idx}): {e}")
                # ä½¿ç”¨é»˜è®¤è¯„åˆ†
                scored_batch = []
                for item in batch:
                    item_copy = item.copy()
                    item_copy['score'] = 0.5
                    item_copy['relevance'] = 0.5
                    item_copy['importance'] = 0.5
                    item_copy['timeliness'] = 0.5
                    item_copy['reliability'] = 0.5
                    scored_batch.append(item_copy)
                with results_lock:
                    results_dict[batch_start_idx] = scored_batch
                
                with status_lock:
                    worker_status[worker_id] = {'batch': batch_num, 'status': 'å¤±è´¥'}
                progress.update(task_ids[worker_id], completed=100,
                              description=f"[red]Worker {worker_id+1}[/] æ‰¹æ¬¡ {batch_num} âœ—")
            
            # æ›´æ–°æ€»è¿›åº¦
            progress.advance(overall_task)
        
        # ä½¿ç”¨ rich Progress æ˜¾ç¤ºå¤šè¿›åº¦æ¡
        with Progress(
            SpinnerColumn(),
            TextColumn("[bold blue]{task.description}"),
            BarColumn(bar_width=30),
            TaskProgressColumn(),
            TimeElapsedColumn(),
            console=console,
            transient=False,
        ) as progress:
            
            # åˆ›å»ºæ€»è¿›åº¦ä»»åŠ¡
            overall_task = progress.add_task(
                f"[bold yellow]ğŸ“Š æ€»è¿›åº¦ ({len(items)} æ¡æ•°æ®, {total_batches} æ‰¹æ¬¡)[/]", 
                total=total_batches
            )
            
            # ä¸ºæ¯ä¸ª worker åˆ›å»ºè¿›åº¦æ¡
            task_ids: Dict[int, Any] = {}
            for i in range(max_workers):
                task_id = progress.add_task(
                    f"[dim]Worker {i+1}[/] ç­‰å¾…ä¸­...", 
                    total=100,
                    start=False
                )
                task_ids[i] = task_id
            
            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # åˆ†é…ä»»åŠ¡ç»™ workers
                futures = []
                for idx, batch_info in enumerate(batches):
                    worker_id = idx % max_workers
                    future = executor.submit(
                        score_single_batch, 
                        batch_info, 
                        worker_id, 
                        progress, 
                        task_ids, 
                        overall_task
                    )
                    futures.append(future)
                
                # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                for future in as_completed(futures):
                    try:
                        future.result()
                    except Exception as e:
                        self.logger.error(f"çº¿ç¨‹æ‰§è¡Œå¼‚å¸¸: {e}")
        
        # æŒ‰åŸå§‹é¡ºåºåˆå¹¶ç»“æœ
        scored_items = []
        for batch_start_idx in sorted(results_dict.keys()):
            scored_items.extend(results_dict[batch_start_idx])
        
        # æ‰“å°å®Œæˆç»Ÿè®¡
        console.print(Panel(
            f"[bold green]âœ… è¯„åˆ†å®Œæˆ[/]\n"
            f"â€¢ æ€»æ•°æ®é‡: {len(scored_items)} æ¡\n"
            f"â€¢ æ‰¹æ¬¡æ•°é‡: {total_batches} æ‰¹\n"
            f"â€¢ å¹¶è¡Œçº¿ç¨‹: {max_workers} ä¸ª",
            title="[bold]è¯„åˆ†ç»Ÿè®¡[/]",
            border_style="green"
        ))
        
        self.logger.info(f"å®Œæˆ {len(scored_items)} æ¡ä¿¡æ¯çš„å¹¶è¡Œè¯„åˆ†")
        return scored_items
    
    def _call_llm_for_scoring(self, items: List[Dict[str, Any]], topics: List[str]) -> List[Dict[str, Any]]:
        """
        è°ƒç”¨å¤§æ¨¡å‹å¯¹ä¿¡æ¯è¿›è¡Œè¯„åˆ†
        
        Args:
            items: ä¿¡æ¯åˆ—è¡¨
            topics: ä¸»é¢˜åˆ—è¡¨
            
        Returns:
            è¯„åˆ†ç»“æœåˆ—è¡¨
        """
        # æ„å»ºprompt
        prompt = self._build_scoring_prompt(items, topics)
        
        # è°ƒç”¨å¤§æ¨¡å‹API
        if self.llm_config.get('provider') == 'qwen':
            return self._call_qwen_api(prompt, task_type='scoring')
        else:
            self.logger.warning(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {self.llm_config.get('provider')}")
            return []
    
    def _build_scoring_prompt(self, items: List[Dict[str, Any]], topics: List[str]) -> str:
        """æ„å»ºè¯„åˆ†prompt"""
        items_text = "\n\n".join([
            f"[ä¿¡æ¯{i+1}]\næ ‡é¢˜: {item.get('title', '')}\næ‘˜è¦: {item.get('snippet', '')[:200]}\næ¥æº: {item.get('source_name', '')}\nå‘å¸ƒæ—¶é—´: {item.get('date_published', '')}"
            for i, item in enumerate(items)
        ])
        
        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¿¡æ¯åˆ†æä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹ä¿¡æ¯è¿›è¡Œå¤šç»´åº¦è¯„åˆ†ã€‚

å…³æ³¨ä¸»é¢˜: {', '.join(topics)}

å¾…è¯„åˆ†ä¿¡æ¯:
{items_text}

è¯·ä»ä»¥ä¸‹å››ä¸ªç»´åº¦å¯¹æ¯æ¡ä¿¡æ¯è¿›è¡Œ0-1ä¹‹é—´çš„è¯„åˆ†:
1. ç›¸å…³æ€§(relevance): ä¸ä¸»é¢˜çš„ç›¸å…³ç¨‹åº¦
2. é‡è¦æ€§(importance): ä¿¡æ¯çš„é‡è¦æ€§å’Œå½±å“åŠ›
3. æ—¶æ•ˆæ€§(timeliness): ä¿¡æ¯çš„æ–°é²œåº¦å’Œæ—¶æ•ˆæ€§
4. å¯é æ€§(reliability): ä¿¡æ¯æ¥æºçš„å¯é æ€§

æœ€åç»™å‡ºç»¼åˆè¯„åˆ†(score)ï¼Œè®¡ç®—å…¬å¼ä¸º:
score = 0.3*relevance + 0.3*importance + 0.2*timeliness + 0.2*reliability

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›è¯„åˆ†ç»“æœï¼ˆä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—ï¼‰:
[
  {{
    "index": 1,
    "relevance": 0.8,
    "importance": 0.7,
    "timeliness": 0.9,
    "reliability": 0.8,
    "score": 0.8,
    "brief_analysis": "ç®€è¦åˆ†æåŸå› "
  }},
  ...
]
"""
        return prompt
    
    def _extract_key_points(self, items: List[Dict[str, Any]], topics: List[str]) -> List[str]:
        """
        æå–å…³é”®è¦ç‚¹
        
        Args:
            items: å·²ç­›é€‰çš„é«˜è´¨é‡ä¿¡æ¯åˆ—è¡¨
            topics: ä¸»é¢˜åˆ—è¡¨
            
        Returns:
            å…³é”®è¦ç‚¹åˆ—è¡¨
        """
        self.logger.info("æå–å…³é”®è¦ç‚¹...")
        
        if not items:
            return []
        
        # é€‰å–è¯„åˆ†æœ€é«˜çš„å‰20æ¡ä¿¡æ¯
        top_items = sorted(items, key=lambda x: x.get('score', 0), reverse=True)[:20]
        
        # æ„å»ºprompt
        items_text = "\n\n".join([
            f"æ ‡é¢˜: {item.get('title', '')}\næ‘˜è¦: {item.get('snippet', '')[:300]}\nè¯„åˆ†: {item.get('score', 0):.2f}"
            for item in top_items
        ])
        
        prompt = f"""åŸºäºä»¥ä¸‹å…³äº"{', '.join(topics)}"çš„é«˜è´¨é‡ä¿¡æ¯ï¼Œè¯·æå–5-10ä¸ªæœ€é‡è¦çš„å…³é”®è¦ç‚¹ã€‚

ä¿¡æ¯å†…å®¹:
{items_text}

è¯·ç”¨ç®€æ´çš„è¯­è¨€åˆ—å‡ºå…³é”®è¦ç‚¹ï¼Œæ¯ä¸ªè¦ç‚¹ä¸€è¡Œï¼Œæ ¼å¼å¦‚ä¸‹:
- è¦ç‚¹1
- è¦ç‚¹2
...
"""
        
        try:
            response = self._call_qwen_api(prompt, task_type='extraction')
            # è§£æè¦ç‚¹
            key_points = [line.strip('- ').strip() for line in response.split('\n') if line.strip().startswith('-')]
            self.logger.info(f"æå–åˆ° {len(key_points)} ä¸ªå…³é”®è¦ç‚¹")
            return key_points
        except Exception as e:
            self.logger.error(f"å…³é”®è¦ç‚¹æå–å¤±è´¥: {e}")
            return []
    
    def _identify_relationships(self, items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        è¯†åˆ«ä¿¡æ¯é—´çš„å…³è”å…³ç³»
        
        Args:
            items: ä¿¡æ¯åˆ—è¡¨
            
        Returns:
            å…³è”å…³ç³»åˆ—è¡¨
        """
        self.logger.info("è¯†åˆ«ä¿¡æ¯å…³è”å…³ç³»...")
        
        # ç®€åŒ–å®ç°ï¼šåŸºäºå…³é”®è¯å…±ç°è¯†åˆ«å…³ç³»
        relationships = []
        
        # æå–æ ‡é¢˜ä¸­çš„å…³é”®è¯
        from collections import Counter
        all_words = []
        for item in items:
            title = item.get('title', '')
            # ç®€å•çš„ä¸­æ–‡åˆ†è¯ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥ä½¿ç”¨jiebaï¼‰
            words = [w for w in title if len(w) > 1]
            all_words.extend(words)
        
        # æ‰¾å‡ºé«˜é¢‘è¯
        word_counts = Counter(all_words)
        top_keywords = [word for word, count in word_counts.most_common(10) if count > 1]
        
        self.logger.info(f"è¯†åˆ«åˆ° {len(top_keywords)} ä¸ªå…³é”®è¯")
        
        return relationships
    
    def _generate_overall_analysis(self, items: List[Dict[str, Any]], key_points: List[str], 
                                   topics: List[str], statistics: Dict[str, Any]) -> str:
        """
        ç”Ÿæˆæ€»ä½“åˆ†æï¼šæ•´åˆæ‰€æœ‰ä¿¡æ¯ï¼Œåˆ†æä¸»é¢˜çš„æ€»ä½“è¶‹åŠ¿å’Œæ´å¯Ÿ
        
        Args:
            items: ç­›é€‰åçš„é«˜è´¨é‡ä¿¡æ¯åˆ—è¡¨
            key_points: æå–çš„å…³é”®è¦ç‚¹
            topics: ä¸»é¢˜åˆ—è¡¨
            statistics: ç»Ÿè®¡ä¿¡æ¯
            
        Returns:
            æ€»ä½“åˆ†ææ–‡æœ¬
        """
        self.logger.info("ç”Ÿæˆæ€»ä½“åˆ†æ...")
        
        if not items:
            return "æš‚æ— è¶³å¤Ÿæ•°æ®è¿›è¡Œæ€»ä½“åˆ†æã€‚"
        
        # é€‰å–è¯„åˆ†æœ€é«˜çš„å‰15æ¡ä¿¡æ¯ç”¨äºåˆ†æ
        top_items = sorted(items, key=lambda x: x.get('score', 0), reverse=True)[:15]
        
        # æ„å»ºä¿¡æ¯æ‘˜è¦
        items_summary = "\n".join([
            f"- {item.get('title', '')[:80]}... (æ¥æº: {item.get('source_name', '')}, è¯„åˆ†: {item.get('score', 0):.2f})"
            for item in top_items
        ])
        
        # å…³é”®è¦ç‚¹æ‘˜è¦
        key_points_text = "\n".join([f"- {point}" for point in key_points[:8]]) if key_points else "æš‚æ— å…³é”®è¦ç‚¹"
        
        # ç»Ÿè®¡ä¿¡æ¯
        source_info = ", ".join([f"{k}: {v}æ¡" for k, v in statistics.get('source_distribution', {}).items()])
        date_info = statistics.get('date_distribution', {})
        date_range = f"{min(date_info.keys()) if date_info else 'N/A'} è‡³ {max(date_info.keys()) if date_info else 'N/A'}"
        
        prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ä¿¡æ¯åˆ†æä¸“å®¶å’Œè¡Œä¸šè§‚å¯Ÿå®¶ã€‚è¯·åŸºäºä»¥ä¸‹æ”¶é›†åˆ°çš„ä¿¡æ¯ï¼Œå¯¹ã€Œ{", ".join(topics)}ã€è¿™ä¸€ä¸»é¢˜è¿›è¡Œæ·±åº¦æ€»ä½“åˆ†æã€‚

## é‡‡é›†ä¿¡æ¯æ¦‚å†µ
- æ€»ä¿¡æ¯æ•°: {statistics.get('total_count', 0)} æ¡
- å¹³å‡è´¨é‡è¯„åˆ†: {statistics.get('average_score', 0):.2f}
- ä¿¡æ¯æ¥æº: {source_info}
- æ—¶é—´èŒƒå›´: {date_range}

## é‡ç‚¹ä¿¡æ¯æ‘˜è¦
{items_summary}

## å·²æå–çš„å…³é”®è¦ç‚¹
{key_points_text}

---

è¯·ç”Ÿæˆä¸€ä»½ã€Œæ™ºè§ˆæ€»ä½“åˆ†æã€ï¼ŒåŒ…å«ä»¥ä¸‹å†…å®¹ï¼š

1. **æ€»ä½“æ€åŠ¿åˆ¤æ–­**: è¯¥ä¸»é¢˜åœ¨è¿‘æœŸçš„æ€»ä½“å‘å±•æ€åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆä¸Šå‡/ä¸‹é™/å¹³ç¨³/æ³¢åŠ¨ï¼‰

2. **æ ¸å¿ƒå‘ç°**: æœ€é‡è¦çš„3-5ä¸ªå‘ç°æ˜¯ä»€ä¹ˆï¼Ÿè¿™äº›å‘ç°ä¹‹é—´æœ‰ä»€ä¹ˆå…³è”ï¼Ÿ

3. **çƒ­ç‚¹å­ä¸»é¢˜**: è¯¥ä¸»é¢˜ä¸‹æœ€å—å…³æ³¨çš„2-3ä¸ªå­æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ

4. **è¶‹åŠ¿é¢„åˆ¤**: åŸºäºå½“å‰æ•°æ®ï¼Œè¯¥ä¸»é¢˜æœªæ¥1-2å‘¨å¯èƒ½çš„å‘å±•æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ

5. **å…³æ³¨å»ºè®®**: å¯¹äºå…³æ³¨è¯¥ä¸»é¢˜çš„äººï¼Œåº”è¯¥é‡ç‚¹å…³æ³¨ä»€ä¹ˆï¼Ÿ

è¯·ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€ï¼Œæä¾›æœ‰æ´å¯ŸåŠ›çš„åˆ†æã€‚æ¯ä¸ªéƒ¨åˆ†ç”¨ Markdown æ ¼å¼è¾“å‡ºï¼Œæ€»å­—æ•°æ§åˆ¶åœ¨ 600-1000 å­—ã€‚
"""
        
        try:
            response = self._call_qwen_api(prompt, task_type='analysis')
            if response and not response.startswith('æ¨¡æ‹Ÿ'):
                self.logger.info("æ€»ä½“åˆ†æç”Ÿæˆå®Œæˆ")
                return response
            else:
                return self._generate_fallback_analysis(topics, statistics, key_points)
        except Exception as e:
            self.logger.error(f"æ€»ä½“åˆ†æç”Ÿæˆå¤±è´¥: {e}")
            return self._generate_fallback_analysis(topics, statistics, key_points)
    
    def _generate_fallback_analysis(self, topics: List[str], statistics: Dict[str, Any], key_points: List[str]) -> str:
        """
        ç”Ÿæˆå¤‡ç”¨çš„æ€»ä½“åˆ†æï¼ˆå½“LLMä¸å¯ç”¨æ—¶ï¼‰
        """
        topic_str = "ã€".join(topics)
        total = statistics.get('total_count', 0)
        avg_score = statistics.get('average_score', 0)
        sources = list(statistics.get('source_distribution', {}).keys())
        
        analysis = f"""### æ€»ä½“æ€åŠ¿

åŸºäºæœ¬æ¬¡é‡‡é›†çš„ {total} æ¡é«˜è´¨é‡ä¿¡æ¯ï¼ˆå¹³å‡è¯„åˆ† {avg_score:.2f}ï¼‰ï¼Œã€Œ{topic_str}ã€ä¸»é¢˜åœ¨è¿‘æœŸä¿æŒè¾ƒé«˜çš„å…³æ³¨åº¦ã€‚

### æ ¸å¿ƒå‘ç°

"""
        if key_points:
            for i, point in enumerate(key_points[:5], 1):
                analysis += f"{i}. {point}\n"
        else:
            analysis += "æš‚æ— è¶³å¤Ÿæ•°æ®æå–æ ¸å¿ƒå‘ç°ã€‚\n"
        
        analysis += f"""
### ä¿¡æ¯æ¥æºåˆ†å¸ƒ

æœ¬æ¬¡åˆ†æçš„ä¿¡æ¯ä¸»è¦æ¥è‡ª: {"ã€".join(sources) if sources else "æš‚æ— æ•°æ®"}

### å…³æ³¨å»ºè®®

å»ºè®®æŒç»­å…³æ³¨è¯¥ä¸»é¢˜çš„æœ€æ–°å‘å±•ï¼Œç‰¹åˆ«æ˜¯é«˜è¯„åˆ†ä¿¡æ¯æºçš„æ›´æ–°ã€‚
"""
        return analysis
    
    def _compute_statistics(self, items: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            items: ä¿¡æ¯åˆ—è¡¨
            
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        from collections import Counter
        
        # ä¿¡æ¯æºåˆ†å¸ƒ
        source_distribution = Counter([item.get('source', 'Unknown') for item in items])
        
        # æŒ‰æ—¥æœŸç»Ÿè®¡
        date_distribution = Counter()
        for item in items:
            try:
                date_str = item.get('date_published', '')
                date = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                date_key = date.strftime('%Y-%m-%d')
                date_distribution[date_key] += 1
            except:
                pass
        
        # è¯„åˆ†åˆ†å¸ƒ
        score_ranges = {'0.6-0.7': 0, '0.7-0.8': 0, '0.8-0.9': 0, '0.9-1.0': 0}
        for item in items:
            score = item.get('score', 0)
            if 0.6 <= score < 0.7:
                score_ranges['0.6-0.7'] += 1
            elif 0.7 <= score < 0.8:
                score_ranges['0.7-0.8'] += 1
            elif 0.8 <= score < 0.9:
                score_ranges['0.8-0.9'] += 1
            elif score >= 0.9:
                score_ranges['0.9-1.0'] += 1
        
        statistics = {
            'total_count': len(items),
            'source_distribution': dict(source_distribution),
            'date_distribution': dict(sorted(date_distribution.items())),
            'score_distribution': score_ranges,
            'average_score': sum(item.get('score', 0) for item in items) / len(items) if items else 0
        }
        
        return statistics
    
    def _call_qwen_api(self, prompt: str, task_type: str = 'general') -> Any:
        """
        è°ƒç”¨é€šä¹‰åƒé—®API
        
        Args:
            prompt: æç¤ºè¯
            task_type: ä»»åŠ¡ç±»å‹
            
        Returns:
            APIå“åº”ç»“æœ
        """
        api_key = self.llm_config.get('api_key')
        
        if api_key == "YOUR_LLM_API_KEY":
            self.logger.warning("LLM APIå¯†é’¥æœªé…ç½®ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
            # è¿”å›æ¨¡æ‹Ÿæ•°æ®
            if task_type == 'scoring':
                return [{'index': i+1, 'relevance': 0.7, 'importance': 0.7, 
                        'timeliness': 0.7, 'reliability': 0.7, 'score': 0.7,
                        'brief_analysis': 'æ¨¡æ‹Ÿè¯„åˆ†'} for i in range(10)]
            return "æ¨¡æ‹Ÿå“åº”"
        
        headers = {
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json'
        }
        
        data = {
            'model': self.llm_config.get('model', 'qwen-plus'),
            'input': {
                'messages': [
                    {'role': 'system', 'content': 'ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¿¡æ¯åˆ†æä¸“å®¶ã€‚'},
                    {'role': 'user', 'content': prompt}
                ]
            },
            'parameters': {
                'max_tokens': self.llm_config.get('max_tokens', 2000),
                'temperature': self.llm_config.get('temperature', 0.7)
            }
        }
        
        try:
            response = requests.post(
                self.llm_config.get('endpoint'),
                headers=headers,
                json=data,
                timeout=60
            )
            response.raise_for_status()
            result = response.json()
            
            # æå–å›å¤å†…å®¹
            content = result.get('output', {}).get('choices', [{}])[0].get('message', {}).get('content', '')
            
            # å¦‚æœæ˜¯è¯„åˆ†ä»»åŠ¡ï¼Œè§£æJSON
            if task_type == 'scoring':
                try:
                    # å°è¯•æå–JSONéƒ¨åˆ†
                    json_start = content.find('[')
                    json_end = content.rfind(']') + 1
                    if json_start >= 0 and json_end > json_start:
                        json_str = content[json_start:json_end]
                        return json.loads(json_str)
                except json.JSONDecodeError as e:
                    self.logger.error(f"JSONè§£æå¤±è´¥: {e}")
                    return []
            
            return content
            
        except Exception as e:
            self.logger.error(f"è°ƒç”¨LLM APIå¤±è´¥: {e}")
            raise


if __name__ == "__main__":
    # æµ‹è¯•åˆ†æå™¨
    from config import get_config
    from logger import LoggerManager
    
    config = get_config()
    log_manager = LoggerManager(config)
    analyzer = InformationAnalyzer(config, log_manager)
    
    # æ¨¡æ‹Ÿæµ‹è¯•æ•°æ®
    test_items = [
        {
            'title': 'GPT-4å‘å¸ƒé‡å¤§æ›´æ–°',
            'snippet': 'OpenAIå‘å¸ƒäº†GPT-4çš„æœ€æ–°ç‰ˆæœ¬...',
            'source': 'NewsAPI',
            'source_name': 'TechCrunch',
            'date_published': '2025-12-20T10:00:00Z'
        }
    ]
    
    result = analyzer.analyze_items(test_items, ['äººå·¥æ™ºèƒ½'])
    print(f"åˆ†æå®Œæˆï¼Œç­›é€‰å‡º {len(result['filtered_items'])} æ¡é«˜è´¨é‡ä¿¡æ¯")
